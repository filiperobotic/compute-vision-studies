"""
Pruning YOLOv11 com NVIDIA Model Optimizer

Baseado em: https://y-t-g.github.io/tutorials/yolo-prune/

IMPORTANTE: Este c√≥digo requer a branch especial do Ultralytics com suporte ao ModelOpt.

Requisitos:
    pip install nvidia-modelopt[torch]
    pip install git+https://github.com/ultralytics/ultralytics@qat-nvidia

NOTA: Se encontrar erros de "Inconsistent keys in config", tente:
    1. Deletar checkpoints antigos: rm modelopt_*.pth
    2. Usar target menos agressivo (ex: 80% ao inv√©s de 66%)
    3. Reduzir max_iter_data_loader (ex: 10 ao inv√©s de 20)
    4. Usar modelo menor (yolo11m ou yolo11s ao inv√©s de yolo11x)
    
TARGETS RECOMENDADOS POR MODELO:
    - YOLOv11n/s: 50-60% (pruning agressivo funciona bem)
    - YOLOv11m:   60-70% (pruning moderado)
    - YOLOv11l:   70-80% (pruning conservador)
    - YOLOv11x:   80-85% (muito conservador - modelo j√° √© grande)
"""

import torch
from ultralytics import YOLO
import modelopt.torch.prune as mtp
from ultralytics.utils import LOGGER
from ultralytics.utils.torch_utils import ModelEMA
import math


class PrunedTrainer:
    """
    Trainer customizado que aplica pruning com NVIDIA ModelOpt antes do treinamento
    """
    
    def __init__(self, base_trainer_class):
        """
        Args:
            base_trainer_class: Classe do trainer base do YOLO
        """
        self.base_trainer_class = base_trainer_class
    
    def __call__(self, *args, **kwargs):
        """
        Cria uma nova classe que herda do trainer base
        """
        base_trainer = self.base_trainer_class
        
        class CustomPrunedTrainer(base_trainer):
            def _setup_train(self):
                """
                Setup modificado que aplica pruning antes do treinamento
                """
                import os
                import time
                
                # Chama setup original
                super()._setup_train()
                
                LOGGER.info("="*60)
                LOGGER.info("Iniciando processo de pruning com NVIDIA ModelOpt...")
                LOGGER.info("="*60)
                
                # Remove checkpoints antigos que podem causar conflitos
                checkpoint_files = [
                    "modelopt_fastnas_search_checkpoint.pth",
                    "modelopt_fastnas_search_checkpoint_alt.pth"
                ]
                for ckpt_file in checkpoint_files:
                    if os.path.exists(ckpt_file):
                        try:
                            os.remove(ckpt_file)
                            LOGGER.info(f"Removido checkpoint antigo: {ckpt_file}")
                        except:
                            pass
                
                # Usa timestamp para checkpoint √∫nico
                checkpoint_name = f"modelopt_fastnas_{int(time.time())}.pth"
                
                # Fun√ß√£o para coletar batches
                def collect_func(batch):
                    return self.preprocess_batch(batch)["img"]
                
                # Fun√ß√£o de score (fitness) para avaliar subnets
                def score_func(model):
                    model.eval()
                    # Desabilita salvamento durante avalia√ß√£o
                    save_orig = self.validator.args.save
                    plots_orig = self.validator.args.plots
                    verbose_orig = self.validator.args.verbose
                    
                    self.validator.args.save = False
                    self.validator.args.plots = False
                    self.validator.args.verbose = False
                    self.validator.is_coco = False
                    
                    metrics = self.validator(model=model)
                    
                    # Restaura configura√ß√µes
                    self.validator.args.save = save_orig
                    self.validator.args.plots = plots_orig
                    self.validator.args.verbose = verbose_orig
                    
                    # Retorna fitness (pode ser dict ou objeto com .fitness)
                    if isinstance(metrics, dict):
                        return metrics.get("fitness", 0.0)
                    else:
                        return metrics.fitness
                
                # Configura√ß√µes de pruning
                # Para YOLO11X, targets mais realistas s√£o 80-85%
                # Para modelos menores, pode usar 60-70%
                prune_constraints = {"flops": "80%"}
                
                LOGGER.info(f"Target de pruning: {prune_constraints}")
                LOGGER.info("NOTA: Para YOLOv11X, targets muito agressivos (<75%) podem falhar")
                LOGGER.info("      Use modelos menores (M, S, N) para pruning mais agressivo")
                
                # Desabilita fusing (necess√°rio para subnet search)
                self.model.is_fused = lambda: True
                
                # Cria dummy input
                dummy_input = torch.randn(
                    1, 3, self.args.imgsz, self.args.imgsz
                ).to(self.device)
                
                # Aplica pruning
                LOGGER.info("Executando subnet search (pode demorar)...")
                
                self.model, prune_results = mtp.prune(
                    model=self.model,
                    mode="fastnas",
                    constraints=prune_constraints,
                    dummy_input=dummy_input,
                    config={
                        "score_func": score_func,
                        "checkpoint": checkpoint_name,  # Nome √∫nico por execu√ß√£o
                        "data_loader": self.train_loader,
                        "collect_func": collect_func,
                        "max_iter_data_loader": 20,  # Use 50 para melhores resultados (requer mais RAM)
                        "verbose": 2,  # Mais logs para debug
                    },
                )
                
                LOGGER.info("="*60)
                LOGGER.info("Pruning aplicado com sucesso!")
                LOGGER.info("="*60)
                
                # Move modelo para device
                self.model.to(self.device)
                
                # Recria EMA
                self.ema = ModelEMA(self.model)
                
                # Recria optimizer e scheduler
                weight_decay = (
                    self.args.weight_decay * 
                    self.batch_size * 
                    self.accumulate / 
                    self.args.nbs
                )
                
                iterations = (
                    math.ceil(
                        len(self.train_loader.dataset) / 
                        max(self.batch_size, self.args.nbs)
                    ) * self.epochs
                )
                
                self.optimizer = self.build_optimizer(
                    model=self.model,
                    name=self.args.optimizer,
                    lr=self.args.lr0,
                    momentum=self.args.momentum,
                    decay=weight_decay,
                    iterations=iterations,
                )
                
                LOGGER.info("Optimizer e scheduler recriados")
                LOGGER.info("="*60)
                
                # Salva refer√™ncia ao modelo pruned para exporta√ß√£o posterior
                self.pruned_model_for_export = self.model
            
            def final_eval(self):
                """
                Avalia√ß√£o final modificada para evitar erro de restore
                """
                try:
                    # Tenta avalia√ß√£o normal
                    super().final_eval()
                except RuntimeError as e:
                    if "Inconsistent keys in config" in str(e):
                        LOGGER.warning("Erro ao carregar checkpoint para valida√ß√£o final")
                        LOGGER.warning("Pulando valida√ß√£o final - use model.val() manualmente")
                        # Define m√©tricas vazias para n√£o quebrar o fluxo
                        from collections import defaultdict
                        self.metrics = defaultdict(float)
                    else:
                        raise
            
            def teardown(self):
                """
                Salva modelo em formato compat√≠vel ap√≥s treino
                """
                import torch
                import modelopt.torch.opt as mto
                
                try:
                    # Salva modelo pruned limpo
                    LOGGER.info("="*60)
                    LOGGER.info("Exportando modelo pruned para formato limpo...")
                    
                    # Cria checkpoint limpo
                    clean_path = str(self.save_dir / "weights" / "pruned_clean.pt")
                    
                    # Finaliza otimiza√ß√£o ModelOpt
                    modelopt_state = mto.modelopt_state(self.pruned_model_for_export)
                    
                    clean_ckpt = {
                        "model": self.pruned_model_for_export,
                        "epoch": self.epoch,
                        "best_fitness": self.best_fitness,
                        "train_args": {k: v for k, v in vars(self.args).items()},
                    }
                    
                    torch.save(clean_ckpt, clean_path)
                    LOGGER.info(f"‚úì Modelo limpo salvo em: {clean_path}")
                    
                except Exception as e:
                    LOGGER.warning(f"Erro ao exportar modelo limpo: {e}")
                
                # Chama teardown original
                try:
                    super().teardown()
                except:
                    pass
        
        return CustomPrunedTrainer(*args, **kwargs)


def train_with_pruning(
    model_path="yolo11n.pt",
    data_yaml="coco128.yaml",
    epochs=50,
    imgsz=640,
    batch=16,
    flops_target="66%",
    project="runs/prune",
    name="pruned_model"
):
    """
    Treina modelo YOLO com pruning usando NVIDIA ModelOpt
    
    Args:
        model_path: Caminho para modelo YOLO
        data_yaml: Dataset YAML
        epochs: N√∫mero de √©pocas
        imgsz: Tamanho da imagem
        batch: Batch size
        flops_target: Target de FLOPs (ex: "30%", "50%", "66%")
        project: Diret√≥rio do projeto
        name: Nome do experimento
    
    Returns:
        Resultados do treinamento
    """
    print("\n" + "="*70)
    print("YOLO PRUNING COM NVIDIA MODEL OPTIMIZER")
    print("="*70)
    
    # Carrega modelo
    print(f"\nCarregando modelo: {model_path}")
    model = YOLO(model_path)
    
    # Info do modelo original
    print("\nMODELO ORIGINAL:")
    model.info()
    
    # Obt√©m classe do trainer
    trainer_class = model.task_map[model.task]["trainer"]
    
    # Cria trainer com pruning
    pruned_trainer = PrunedTrainer(trainer_class)
    
    # Treina com pruning
    print(f"\n{'='*70}")
    print(f"INICIANDO TREINAMENTO COM PRUNING")
    print(f"Target: {flops_target} FLOPs")
    print(f"Epochs: {epochs}")
    print(f"{'='*70}\n")
    
    try:
        results = model.train(
            data=data_yaml,
            epochs=epochs,
            imgsz=imgsz,
            batch=batch,
            trainer=pruned_trainer,
            project=project,
            name=name,
            verbose=True
        )
    except RuntimeError as e:
        if "Inconsistent keys in config" in str(e):
            print("\n‚ö†Ô∏è  AVISO: Erro ao carregar checkpoint final (esperado)")
            print("    O treinamento foi conclu√≠do com sucesso!")
            print(f"    Modelo salvo em: {project}/{name}/weights/")
            results = None
        else:
            raise
    
    print("\n" + "="*70)
    print("TREINAMENTO COMPLETO!")
    print("="*70)
    
    # Retorna caminho do modelo ao inv√©s de results
    model_dir = f"{project}/{name}/weights"
    return model_dir


def export_pruned_model(checkpoint_path, output_path="pruned_model_exported.pt"):
    """
    Exporta modelo pruned removendo depend√™ncias do ModelOpt
    
    Args:
        checkpoint_path: Caminho para checkpoint .pt
        output_path: Caminho para salvar modelo exportado
    """
    import torch
    import modelopt.torch.opt as mto
    from ultralytics.nn.tasks import DetectionModel
    
    print(f"\n{'='*70}")
    print("EXPORTANDO MODELO PRUNED")
    print(f"{'='*70}")
    
    try:
        # Carrega checkpoint
        print(f"Carregando: {checkpoint_path}")
        ckpt = torch.load(checkpoint_path, map_location='cpu')
        
        if "modelopt_state" in ckpt:
            print("Detectado modelo com ModelOpt")
            
            # Reconstr√≥i modelo da configura√ß√£o YAML
            model = DetectionModel(ckpt["yaml"], verbose=False)
            model.names = ckpt["names"]
            model.nc = ckpt["nc"]
            
            # Restaura estado do ModelOpt
            with torch.no_grad():
                mto.restore_from_modelopt_state(model, ckpt["modelopt_state"])
                model.load_state_dict(ckpt["state_dict"])
            
            # Exporta modelo "limpo" (sem ModelOpt)
            print("Exportando modelo sem depend√™ncias ModelOpt...")
            mto.modelopt_state(model)  # Finaliza otimiza√ß√£o
            
            # Salva novo checkpoint limpo
            clean_ckpt = {
                "model": model,
                "epoch": ckpt.get("epoch", -1),
                "best_fitness": ckpt.get("best_fitness", 0.0),
                "train_args": ckpt.get("train_args", {}),
            }
            
            torch.save(clean_ckpt, output_path)
            print(f"‚úì Modelo exportado: {output_path}")
            
            # Info do modelo
            print(f"\nInforma√ß√µes do modelo exportado:")
            print(f"  Par√¢metros: {sum(p.numel() for p in model.parameters()):,}")
            
            return output_path
        else:
            print("Modelo n√£o usa ModelOpt, copiando diretamente...")
            import shutil
            shutil.copy(checkpoint_path, output_path)
            return output_path
            
    except Exception as e:
        print(f"‚ùå Erro ao exportar: {e}")
        print("\nTentando m√©todo alternativo...")
        
        # M√©todo alternativo: carrega com YOLO e salva novamente
        try:
            model = YOLO(checkpoint_path)
            model.export(format='torchscript', simplify=True)
            print("‚úì Exportado como TorchScript")
            return checkpoint_path.replace('.pt', '.torchscript')
        except:
            return None


def evaluate_pruned_model(model_path, data_yaml="coco128.yaml"):
    """
    Avalia modelo pruned
    
    Args:
        model_path: Caminho para modelo pruned
        data_yaml: Dataset YAML
    """
    print("\n" + "="*70)
    print("AVALIANDO MODELO PRUNED")
    print("="*70)
    
    model = YOLO(model_path)
    
    print("\nINFO DO MODELO:")
    model.info()
    
    print("\nM√âTRICAS DE VALIDA√á√ÉO:")
    metrics = model.val(data=data_yaml)
    
    return metrics


def compare_models(original_path, pruned_path, data_yaml="coco128.yaml"):
    """
    Compara modelo original vs pruned
    
    Args:
        original_path: Caminho modelo original
        pruned_path: Caminho modelo pruned
        data_yaml: Dataset YAML
    """
    print("\n" + "="*70)
    print("COMPARA√á√ÉO: ORIGINAL vs PRUNED")
    print("="*70)
    
    # Carrega modelos
    original = YOLO(original_path)
    pruned = YOLO(pruned_path)
    
    print("\nüìä MODELO ORIGINAL:")
    print("-" * 70)
    original.info()
    
    print("\nüìä MODELO PRUNED:")
    print("-" * 70)
    pruned.info()
    
    # M√©tricas
    print("\nüìà AVALIANDO PERFORMANCE...")
    print("-" * 70)
    
    print("\nOriginal:")
    orig_metrics = original.val(data=data_yaml, verbose=False)
    
    print("\nPruned:")
    pruned_metrics = pruned.val(data=data_yaml, verbose=False)
    
    # Resumo
    print("\n" + "="*70)
    print("üìä RESUMO DA COMPARA√á√ÉO")
    print("="*70)
    
    print("\nmAP50:")
    print(f"  Original: {orig_metrics.box.map50:.4f}")
    print(f"  Pruned:   {pruned_metrics.box.map50:.4f}")
    print(f"  Diferen√ßa: {pruned_metrics.box.map50 - orig_metrics.box.map50:+.4f}")
    
    print("\nmAP50-95:")
    print(f"  Original: {orig_metrics.box.map:.4f}")
    print(f"  Pruned:   {pruned_metrics.box.map:.4f}")
    print(f"  Diferen√ßa: {pruned_metrics.box.map - orig_metrics.box.map:+.4f}")
    
    # Tamanho dos arquivos
    import os
    orig_size = os.path.getsize(original_path) / (1024**2)
    pruned_size = os.path.getsize(pruned_path) / (1024**2)
    
    print("\nTamanho do arquivo:")
    print(f"  Original: {orig_size:.2f} MB")
    print(f"  Pruned:   {pruned_size:.2f} MB")
    print(f"  Redu√ß√£o:  {(1 - pruned_size/orig_size)*100:.2f}%")
    
    print("\n" + "="*70)


# Exemplo de uso
if __name__ == "__main__":
    import os
    import glob
    
    # LIMPA CHECKPOINTS ANTIGOS ANTES DE COME√áAR
    print("\nüßπ Limpando checkpoints antigos...")
    old_checkpoints = glob.glob("modelopt_*.pth")
    for ckpt in old_checkpoints:
        try:
            os.remove(ckpt)
            print(f"   Removido: {ckpt}")
        except:
            pass
    
    # Configura√ß√µes
    MODEL_PATH = "yolo11m.pt"
    DATA_YAML = "data.yaml"  # Use seu dataset aqui
    EPOCHS = 50  # Aumente para seu dataset real
    BATCH_SIZE = 16
    FLOPS_TARGET = "80%"  # Para YOLO11X, use 80-85% (mais conservador)
                          # Para YOLO11M/S/N, pode usar 60-70%
    
    # 1. Treina com pruning
    print("\n" + "="*70)
    print("ETAPA 1: TREINAMENTO COM PRUNING")
    print("="*70)
    
    model_dir = train_with_pruning(
        model_path=MODEL_PATH,
        data_yaml=DATA_YAML,
        epochs=EPOCHS,
        batch=BATCH_SIZE,
        flops_target=FLOPS_TARGET,
        project="runs/prune",
        name="yolo11x_pruned"
    )
    
    # 2. Tenta usar modelo limpo exportado
    print("\n" + "="*70)
    print("ETAPA 2: VALIDA√á√ÉO DO MODELO PRUNED")
    print("="*70)
    
    # Primeiro tenta o modelo limpo exportado durante treino
    clean_model_path = f"{model_dir}/pruned_clean.pt"
    last_model_path = f"{model_dir}/last.pt"
    best_model_path = f"{model_dir}/best.pt"
    
    import os
    
    if os.path.exists(clean_model_path):
        print(f"‚úì Encontrado modelo limpo: {clean_model_path}")
        model_to_eval = clean_model_path
    else:
        print(f"‚ö†Ô∏è  Modelo limpo n√£o encontrado, tentando exportar...")
        # Tenta exportar o last.pt
        try:
            exported = export_pruned_model(last_model_path, "yolo11x_pruned_exported.pt")
            if exported:
                model_to_eval = exported
            else:
                model_to_eval = None
        except Exception as e:
            print(f"‚ùå Erro ao exportar: {e}")
            model_to_eval = None
    
    # Avalia se conseguiu modelo v√°lido
    if model_to_eval:
        print(f"\nüìä Avaliando: {model_to_eval}")
        try:
            metrics = evaluate_pruned_model(model_to_eval, DATA_YAML)
            
            # 3. Compara com original
            print("\n" + "="*70)
            print("ETAPA 3: COMPARA√á√ÉO COM MODELO ORIGINAL")
            print("="*70)
            compare_models(MODEL_PATH, model_to_eval, DATA_YAML)
        except Exception as e:
            print(f"‚ö†Ô∏è  Erro na avalia√ß√£o: {e}")
            print(f"    Mas o modelo est√° salvo em: {model_dir}")
    else:
        print("\n‚ö†Ô∏è  N√£o foi poss√≠vel avaliar automaticamente")
        print(f"    Modelos salvos em: {model_dir}")
        print("\nüí° Para usar o modelo manualmente:")
        print("    1. Exporte para ONNX ou TensorRT:")
        print(f"       yolo export model={last_model_path} format=onnx")
        print("    2. Ou use diretamente com cautela:")
        print(f"       model = YOLO('{last_model_path}')")
    
    # 4. Instru√ß√µes finais
    print("\n" + "="*70)
    print("‚úÖ PROCESSO COMPLETO!")
    print("="*70)
    
    print("\nüí° PR√ìXIMOS PASSOS:")
    print("  1. Exporte para TensorRT FP16 para ganhos de velocidade:")
    print("     pruned_model.export(format='engine', half=True)")
    print("")
    print("  2. Para carregar o modelo pruned posteriormente:")
    print("     from ultralytics import YOLO")
    print(f"     model = YOLO('{pruned_model_path}')")
    print("")
    print("  3. Ajuste FLOPS_TARGET para diferentes n√≠veis de compress√£o:")
    print("     YOLOv11n/s: 50-60% (agressivo)")
    print("     YOLOv11m:   60-70% (moderado)")  
    print("     YOLOv11l:   70-80% (conservador)")
    print("     YOLOv11x:   80-85% (muito conservador)")
    print("")
    print("  4. IMPORTANTE: Use seu dataset completo para melhores resultados")
    print(f"     (este exemplo usa {DATA_YAML} apenas para demonstra√ß√£o)")
    
    print("\n" + "="*70)